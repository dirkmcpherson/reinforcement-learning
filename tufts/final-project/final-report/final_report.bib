
@inproceedings{bhattacharjee_community-centered_2019,
	title = {A {Community}-{Centered} {Design} {Framework} for {Robot}-{Assisted} {Feeding} {Systems}},
	abstract = {Robot-assisted feeding (RAF) systems offer enormous poten-tial benefts to community-centered care-giving environments. However, developers of RAF technologies often focus on eval-uating their standard transactional functionality, omitting the impact of such technologies in contexts that extend past the interaction of the robot and food receiver. RAF technologies have complex social, cultural and self-identity implications, since a ”meal” extends well beyond the simple provisioning of nourishment. To better understand these implications we con-ducted a contextual inquiry in an assisted-living community with fve potential care recipients and fve caregivers, as well as interviews with ffteen domain experts including occupa-tional therapists and feeding specialists. Based on our fndings from these studies, we developed a new framework for RAF technologies that formulates this vital task as a community-centered relational service. We then use this framework to qualitatively and quantitatively assess three existing feeding systems and identify areas of improvement. Our work reveals new insights about stakeholders of RAF technologies and pro-vides a roadmap for technology developers to better serve the needs of these stakeholders.},
	publisher = {University of Washington},
	author = {Bhattacharjee, Tapomayukh and E. Cabrera, Maria and Caspi, Anat and Cakmak, Maya and S. Srinivasa, Siddhartha},
	year = {2019}
}

@incollection{griffith_policy_2013,
	title = {Policy {Shaping}: {Integrating} {Human} {Feedback} with {Reinforcement} {Learning}},
	url = {http://papers.nips.cc/paper/5187-policy-shaping-integrating-human-feedback-with-reinforcement-learning.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Griffith, Shane and Subramanian, Kaushik and Scholz, Jonathan and Isbell, Charles L and Thomaz, Andrea L},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {2625--2633}
}

@inproceedings{knox_tamer:_2008,
	title = {{TAMER}: {Training} an {Agent} {Manually} via {Evaluative} {Reinforcement}},
	doi = {10.1109/DEVLRN.2008.4640845},
	booktitle = {2008 7th {IEEE} {International} {Conference} on {Development} and {Learning}},
	author = {Knox, W. Bradley and Stone, P.},
	month = aug,
	year = {2008},
	keywords = {learning (artificial intelligence), Robots, Humans, agent learning process, Approximation algorithms, biocybernetics, decision making, decision theory, evaluative reinforcement, Games, Learning, learning agent training, manual agent training, reward function, scalar reward signals, sequential decision making tasks, Supervised learning, TAMER, Tetris, Training, Training an Agent Manually via Evaluative Reinforcement},
	pages = {292--297}
}

@book{sutton_introduction_1998,
	address = {Cambridge, MA, USA},
	edition = {1st},
	title = {Introduction to {Reinforcement} {Learning}},
	isbn = {0-262-19398-1},
	publisher = {MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {1998}
}

@inproceedings{pilarski_online_2011,
	title = {Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning},
	doi = {10.1109/ICORR.2011.5975338},
	booktitle = {2011 {IEEE} {International} {Conference} on {Rehabilitation} {Robotics}},
	author = {Pilarski, P. M. and Dawson, M. R. and Degris, T. and Fahimi, F. and Carey, J. P. and Sutton, R. S.},
	month = jun,
	year = {2011},
	keywords = {medical robotics, learning (artificial intelligence), Robots, electromyography, Electromyography, Humans, Joints, Training, amputee-specific motions, Artificial Intelligence, artificial limbs, Artificial Limbs, continuous actor-critic reinforcement learning method, electromyograph signals, EMG, information services, intelligent artificial limbs, Learning systems, Models, multifunction myoelectric devices, muscle, myoelectric control approach, myoelectric prosthesis controller, one-dimensional feedback signal, online human training, reinforcement-based machine learning framework, simulated upper-arm robotic prosthesis, sparse human-delivered training signal, Theoretical, Wrist},
	pages = {1--7}
}

@inproceedings{ng_policy_1999,
	title = {Policy invariance under reward transformations: {Theory} and application to reward shaping},
	volume = {99},
	booktitle = {{ICML}},
	author = {Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
	year = {1999},
	pages = {278--287}
}

@inproceedings{gams_-line_2017,
	address = {Cham},
	title = {On-{Line} {Modifications} of {Robotic} {Trajectories}: {Learning}, {Coaching} and {Force} {Vs}. {Position} {Feedback}},
	isbn = {978-3-319-49058-8},
	abstract = {Robotic coaching is a process of modifying the motion of a robot during execution through human intervention, in a manner of a coach. Thus, parts of the motion are changed depending on the instructions which can be provided through different approaches. Methods and analyses of robotic coaching have been described in the literature, including methods relying on physical human-robot interaction. In this paper we analyze one such method, which uses a compliantly controlled robot, to different variations of position and force feedback. Our method is based on changing the motion by changing the dynamic motion primitives' (DMPs) weights. We qualitatively evaluate how such DMP based coaching methods can be made more responsive and what are the benefits of position vs. force feedback. Finally, through the use of position feedback on compliant robots, we show that the inherent properties of DMP learning itself can be used to achieve the same results as when coaching the robot. We present the results of a real-world experiment on a Kuka LWR-4 robot.},
	booktitle = {Advances in {Robot} {Design} and {Intelligent} {Control}},
	publisher = {Springer International Publishing},
	author = {Gams, Andrej and Petrič, Tadej},
	editor = {Rodić, Aleksandar and Borangiu, Theodor},
	year = {2017},
	pages = {20--28}
}

@inproceedings{nemec_human-robot_2014,
	title = {Human-robot cooperation through force adaptation using dynamic motion primitives and iterative learning},
	doi = {10.1109/ROBIO.2014.7090536},
	booktitle = {2014 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics} ({ROBIO} 2014)},
	author = {Nemec, B. and Gams, A. and Deniša, M. and Ude, A.},
	month = dec,
	year = {2014},
	keywords = {Trajectory, Robot sensing systems, human-robot interaction, mobile robots, autonomous agents, bimanual human-robot collaborative task, dynamic movement primitives, Force, force adaptation, force feedback, human intention adaptation, human-robot cooperation, iterative learning control, iterative learning controller, KUKA LWR robots, Null space, object manipulation, Quaternions, robot dynamics, robot trajectory adaptation, robot trajectory representation, safety, sensory feedback, Torque, trajectory control},
	pages = {1439--1444}
}

@article{gams_adaptation_2016,
	title = {Adaptation and coaching of periodic motion primitives through physical and visual interaction},
	volume = {75},
	issn = {0921-8890},
	url = {http://www.sciencedirect.com/science/article/pii/S0921889015001992},
	doi = {https://doi.org/10.1016/j.robot.2015.09.011},
	abstract = {In this paper we propose and evaluate a control system to (1) learn and (2) adapt robot motion for continuous non-rigid contact with the environment. We present the approach in the context of wiping surfaces with robots. Our approach is based on learning by demonstration. First an initial periodic motion, covering the essence of the wiping task, is transferred from a human to a robot. The system extracts and learns one period of motion. Once the user/demonstrator is content with the motion, the robot seeks and establishes contact with a given surface, maintaining a predefined force of contact through force feedback. The shape of the surface is encoded for the complete period of motion, but the robot can adapt to a different surface, perturbations or obstacles. The novelty stems from the fact that the feedforward component is learned and encoded in a dynamic movement primitive. By using the feedforward component, the feedback component is greatly reduced if not completely canceled. Finally, if the user is not satisfied with the periodic pattern, he/she can change parts of motion through predefined gestures or through physical contact in a manner of a tutor or a coach. The complete system thus allows not only a transfer of motion, but a transfer of motion with matching correspondences, i.e. wiping motion is constrained to maintain physical contact with the surface to be wiped. The interface for both learning and adaptation is simple and intuitive and allows for fast and reliable knowledge transfer to the robot. Simulated and real world results in the application domain of wiping a surface are presented on three different robotic platforms. Results of the three robotic platforms, namely a 7 degree-of-freedom Kuka LWR-4 robot, the ARMAR-IIIa humanoid platform and the Sarcos CB-i humanoid robot, depict different methods of adaptation to the environment and coaching.},
	journal = {Robotics and Autonomous Systems},
	author = {Gams, Andrej and Petrič, Tadej and Do, Martin and Nemec, Bojan and Morimoto, Jun and Asfour, Tamim and Ude, Aleš},
	year = {2016},
	keywords = {Coaching, Dynamic movement primitives, Force control, Human–robot interaction},
	pages = {340 -- 351}
}

@inproceedings{gams_learning_2014,
	title = {Learning and adaptation of periodic motion primitives based on force feedback and human coaching interaction},
	doi = {10.1109/HUMANOIDS.2014.7041354},
	booktitle = {2014 {IEEE}-{RAS} {International} {Conference} on {Humanoid} {Robots}},
	author = {Gams, A. and Petric, T. and Nemec, B. and Ude, A.},
	month = nov,
	year = {2014},
	keywords = {Trajectory, human-robot interaction, learning (artificial intelligence), Robots, motion control, Force, force feedback, trajectory control, Acceleration, autonomous exploration, complex robot behavior, control engineering computing, Delays, discrete point-to-point movement, dynamic movement primitive, Force feedback, Force measurement, gesture recognition, human coaching interaction, human tutor, Kuka 7 degree-of-freedom LWR robot, learned trajectory, learning, periodic DMP trajectory, periodic motion primitive, periodic movement, predefined gesture},
	pages = {166--171}
}

@inproceedings{argall_learning_2007,
	address = {New York, NY, USA},
	series = {{HRI} '07},
	title = {Learning by {Demonstration} with {Critique} from a {Human} {Teacher}},
	isbn = {978-1-59593-617-2},
	url = {http://doi.acm.org.ezproxy.library.tufts.edu/10.1145/1228716.1228725},
	doi = {10.1145/1228716.1228725},
	booktitle = {Proceedings of the {ACM}/{IEEE} {International} {Conference} on {Human}-robot {Interaction}},
	publisher = {ACM},
	author = {Argall, Brenna and Browning, Brett and Veloso, Manuela},
	year = {2007},
	note = {event-place: Arlington, Virginia, USA},
	pages = {57--64}
}

@inproceedings{riley_coaching:_2007,
	title = {Coaching: {An} {Approach} to {Efficiently} and {Intuitively} {Create} {Humanoid} {Robot} {Behaviors}},
	doi = {10.1109/ICHR.2006.321330},
	author = {Riley, Marcia and Ude, Ales and Atkeson, Christopher and Cheng, Gordon},
	year = {2007},
	pages = {567 -- 574}
}

@misc{niekum_ros-dmp_nodate,
	title = {ros-dmp},
	url = {http://wiki.ros.org/dmp},
	urldate = {2019-11-19},
	author = {Niekum, Scott}
}

@misc{noauthor_controlling_2019,
	title = {Controlling a 2D {Robotic} {Arm} with {Deep} {Reinforcement} {Learning}},
	url = {https://blog.floydhub.com/robotic-arm-control-deep-reinforcement-learning/},
	urldate = {2019-11-16},
	journal = {FloydHub},
	month = feb,
	year = {2019}
}

@book{zhou_train-robot-arm--scratch_2019,
	title = {train-robot-arm-from-scratch},
	url = {https://github.com/MorvanZhou/train-robot-arm-from-scratch},
	publisher = {GitHub},
	author = {Zhou, Morvan},
	year = {2019}
}

@book{lillicrap_continuous_2015,
	title = {Continuous control with deep reinforcement learning},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	year = {2015}
}

@misc{sutton_tile_nodate,
	title = {Tile {Coding} {Sofware}},
	url = {http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/RLtoolkit/tiles.html},
	urldate = {2019-11-18},
	journal = {Reinforcement Learning and Artificial Intelligence (RLAI)},
	author = {Sutton, R.S}
}

@inproceedings{cui_active_2018,
	title = {Active {Reward} {Learning} from {Critiques}},
	doi = {10.1109/ICRA.2018.8460854},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Cui, Y. and Niekum, S.},
	month = may,
	year = {2018},
	keywords = {Trajectory, Task analysis, learning (artificial intelligence), Robots, control engineering computing, active Bayesian inverse reinforcement learning, active learning, active reward Learning, Bayes methods, critiques, Entropy, labeling process, Learning (artificial intelligence), programming robots, query processing, robot programming, trajectory queries, Uncertainty},
	pages = {6907--6914}
}

@book{cui_uncertainty-aware_2019,
	title = {Uncertainty-{Aware} {Data} {Aggregation} for {Deep} {Imitation} {Learning}},
	author = {Cui, Yuchen and Isele, David and Niekum, Scott and Fujimura, Kikuo},
	year = {2019}
}

@inproceedings{knox_interactively_2009,
	address = {New York, NY, USA},
	series = {K-{CAP} '09},
	title = {Interactively {Shaping} {Agents} via {Human} {Reinforcement}: {The} {TAMER} {Framework}},
	isbn = {978-1-60558-658-8},
	url = {http://doi.acm.org.ezproxy.library.tufts.edu/10.1145/1597735.1597738},
	doi = {10.1145/1597735.1597738},
	booktitle = {Proceedings of the {Fifth} {International} {Conference} on {Knowledge} {Capture}},
	publisher = {ACM},
	author = {Knox, W. Bradley and Stone, Peter},
	year = {2009},
	note = {event-place: Redondo Beach, California, USA},
	keywords = {human teachers, human-agent interaction, learning agents, sequential decision-making, shaping},
	pages = {9--16}
}

@inproceedings{knox_reinforcement_2012,
	address = {Richland, SC},
	series = {{AAMAS} '12},
	title = {Reinforcement {Learning} from {Simultaneous} {Human} and {MDP} {Reward}},
	isbn = {0-9817381-1-7 978-0-9817381-1-6},
	url = {http://dl.acm.org/citation.cfm?id=2343576.2343644},
	booktitle = {Proceedings of the 11th {International} {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems} - {Volume} 1},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Knox, W. Bradley and Stone, Peter},
	year = {2012},
	note = {event-place: Valencia, Spain},
	keywords = {human teachers, human-agent interaction, interactive learning, reinforcement learning, shaping},
	pages = {475--482}
}