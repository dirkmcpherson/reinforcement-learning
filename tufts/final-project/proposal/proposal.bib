
@incollection{griffith_policy_2013,
	title = {Policy {Shaping}: {Integrating} {Human} {Feedback} with {Reinforcement} {Learning}},
	url = {http://papers.nips.cc/paper/5187-policy-shaping-integrating-human-feedback-with-reinforcement-learning.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Griffith, Shane and Subramanian, Kaushik and Scholz, Jonathan and Isbell, Charles L and Thomaz, Andrea L},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {2625--2633}
}

@inproceedings{knox_tamer:_2008,
	title = {{TAMER}: {Training} an {Agent} {Manually} via {Evaluative} {Reinforcement}},
	doi = {10.1109/DEVLRN.2008.4640845},
	booktitle = {2008 7th {IEEE} {International} {Conference} on {Development} and {Learning}},
	author = {Knox, W. Bradley and Stone, P.},
	month = aug,
	year = {2008},
	keywords = {learning (artificial intelligence), Robots, Humans, agent learning process, Approximation algorithms, biocybernetics, decision making, decision theory, evaluative reinforcement, Games, Learning, learning agent training, manual agent training, reward function, scalar reward signals, sequential decision making tasks, Supervised learning, TAMER, Tetris, Training, Training an Agent Manually via Evaluative Reinforcement},
	pages = {292--297}
}

@book{sutton_introduction_1998,
	address = {Cambridge, MA, USA},
	edition = {1st},
	title = {Introduction to {Reinforcement} {Learning}},
	isbn = {0-262-19398-1},
	publisher = {MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {1998}
}

@inproceedings{pilarski_online_2011,
	title = {Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning},
	doi = {10.1109/ICORR.2011.5975338},
	booktitle = {2011 {IEEE} {International} {Conference} on {Rehabilitation} {Robotics}},
	author = {Pilarski, P. M. and Dawson, M. R. and Degris, T. and Fahimi, F. and Carey, J. P. and Sutton, R. S.},
	month = jun,
	year = {2011},
	keywords = {medical robotics, learning (artificial intelligence), Robots, electromyography, Electromyography, Humans, Joints, Training, amputee-specific motions, Artificial Intelligence, artificial limbs, Artificial Limbs, continuous actor-critic reinforcement learning method, electromyograph signals, EMG, information services, intelligent artificial limbs, Learning systems, Models, multifunction myoelectric devices, muscle, myoelectric control approach, myoelectric prosthesis controller, one-dimensional feedback signal, online human training, reinforcement-based machine learning framework, simulated upper-arm robotic prosthesis, sparse human-delivered training signal, Theoretical, Wrist},
	pages = {1--7}
}

@inproceedings{ng_policy_1999,
	title = {Policy invariance under reward transformations: {Theory} and application to reward shaping},
	volume = {99},
	booktitle = {{ICML}},
	author = {Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
	year = {1999},
	pages = {278--287}
}